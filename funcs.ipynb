{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "These are the functions needed in order to run the extraction notebook. **Please do not edit this file as it will break the code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd   # Note that you require geopandas version >= 0.7 that incluse clip see here for installation (https://gis.stackexchange.com/questions/360127/geopandas-0-6-1-installed-instead-of-0-7-0-in-conda-windows-10#)\n",
    "import os\n",
    "import fiona\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from rasterstats import zonal_stats\n",
    "import rasterio\n",
    "from geojson import Feature, Point, FeatureCollection\n",
    "import rasterio.fill\n",
    "from shapely.geometry import shape, mapping\n",
    "import json\n",
    "#from earthpy import clip    clip has been deprecated to geopandas\n",
    "import earthpy.spatial as es\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import gdal\n",
    "import datetime\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import scipy.spatial\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "root.attributes(\"-topmost\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting admin 1 boundary name to clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_admin1_name(clusters, admin_col_name, crs):\n",
    "    # Import layer\n",
    "    messagebox.showinfo('OnSSET', 'Select the admin 1 boundaries')\n",
    "    admin_1 = gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    clusters_support = clusters\n",
    "    \n",
    "    #Project layer to proper crs\n",
    "    admin_1_proj = admin_1.to_crs({ 'init': crs})\n",
    "    \n",
    "    # Apply spatial join \n",
    "    clusters = gpd.sjoin(clusters_support, admin_1_proj[[\"geometry\", admin_col_name]], op='within').drop(['index_right'], axis=1)\n",
    "    clusters.rename(columns = {admin_col_name:'Admin_1'}, inplace = True)\n",
    "    \n",
    "    #Return result\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting IDP & Refugee camps characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_IDPs_RefugeeCamps_status(clusters, camp_name, crs):\n",
    "    # Import layer\n",
    "    messagebox.showinfo('OnSSET', 'Select the layer of IDP')\n",
    "    idp_gdf = gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    clusters_support = clusters\n",
    "    \n",
    "    #Project layer to proper crs\n",
    "    idp_gdf_proj = idp_gdf.to_crs({ 'init': crs})\n",
    "    \n",
    "    # Apply spatial join \n",
    "    cluster_support = gpd.sjoin(clusters, idp_gdf_proj[[\"geometry\", camp_name]]).drop(['index_right'], axis=1)\n",
    "    cluster_support[\"IDP_RefugeeCamp\"] = np.where((cluster_support[camp_name].notnull()), 1, 0)\n",
    "    clusters = pd.merge(clusters, cluster_support[['id', camp_name, \"IDP_RefugeeCamp\"]], on='id', how = 'left')\n",
    "    clusters[\"IDP_RefugeeCamp\"] = clusters[\"IDP_RefugeeCamp\"].fillna(0)\n",
    "\n",
    "    # Rename columns\n",
    "    clusters.rename(columns = {camp_name:'Camp_name'}, inplace = True)\n",
    "    \n",
    "    #Return result\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting No of building per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_buildings_in_clusters(clusters, col_name, crs):\n",
    "    # Import layer\n",
    "    messagebox.showinfo('OnSSET', 'Select the layer of building footprints')\n",
    "    gdf = gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    #Converting polygon buildings to points\n",
    "    gdf_centroids = gpd.GeoDataFrame(gdf,\n",
    "                                     crs=\"EPSG:4326\",\n",
    "                                     geometry=[Point(xy) for xy in zip(gdf.centroid.x, gdf.centroid.y)])\n",
    "    \n",
    "    # Reverting clusters to original crs \n",
    "    clusters_support = clusters[['id', 'geometry']].to_crs({'init': \"EPSG:4326\"})\n",
    "    #clusters_support.id = clusters_support.id.astype(int)\n",
    "    \n",
    "    # Apply spatial join and group by cluster \"id\"\n",
    "    pointsInPolygon = gpd.sjoin(gdf_centroids, clusters_support, how=\"inner\", op='intersects')\n",
    "    pointsInPolygon[col_name]=1\n",
    "    group_by_id = pointsInPolygon.groupby([\"id\"]).sum().reset_index().drop(\"index_right\", axis=1)\n",
    "    \n",
    "    # Merge back to clusters\n",
    "    clusters = pd.merge(clusters, group_by_id[['id', col_name]], on='id', how = 'left')\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    clusters[col_name] = clusters[col_name].fillna(0)\n",
    "    \n",
    "    #Return result\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting No of water points per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_waterpoints_in_clusters(clusters, col_name, crs):\n",
    "    # Import layer\n",
    "    messagebox.showinfo('OnSSET', 'Select the layer of water points')\n",
    "    gdf = gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    # Reverting clusters to original crs \n",
    "    clusters_support = clusters[['id', 'geometry']].to_crs({'init': \"EPSG:4326\"})\n",
    "    \n",
    "    # Apply spatial join and group by cluster \"id\"\n",
    "    pointsInPolygon = gpd.sjoin(gdf, clusters_support, how=\"inner\", op='intersects')\n",
    "    pointsInPolygon[col_name]=1\n",
    "    group_by_id = pointsInPolygon.groupby([\"id\"]).sum().reset_index().drop(\"index_right\", axis=1)\n",
    "    \n",
    "    # Merge back to clusters\n",
    "    clusters = pd.merge(clusters, group_by_id[['id', col_name]], on='id', how = 'left')\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    clusters[col_name] = clusters[col_name].fillna(0)\n",
    "    \n",
    "    #Return result\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_raster(name, method, clusters):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    raster=rasterio.open(filedialog.askopenfilename(filetypes = ((\"rasters\",\"*.tif\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        raster.name,\n",
    "        stats=[method],\n",
    "        prefix=name, geojson_out=True, all_touched=True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Elevation and Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_elevation_and_slope(name, method, clusters, workspace,crs):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    raster=rasterio.open(filedialog.askopenfilename(filetypes = ((\"rasters\",\"*.tif\"),(\"all files\",\"*.*\"))))\n",
    "    \n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        raster.name,\n",
    "        stats=[method],\n",
    "        prefix=name, geojson_out=True, all_touched=True)\n",
    "\n",
    "    gdal.Warp(workspace + r\"\\dem.tif\",raster.name,dstSRS=crs)\n",
    "\n",
    "    def calculate_slope(DEM):\n",
    "        gdal.DEMProcessing(workspace + r'\\slope.tif', DEM, 'slope')\n",
    "        with rasterio.open(workspace + r'\\slope.tif') as dataset:\n",
    "            slope=dataset.read(1)\n",
    "        return slope\n",
    "\n",
    "    slope=calculate_slope(workspace + r\"\\dem.tif\")\n",
    "\n",
    "    slope = rasterio.open(workspace + r'\\slope.tif')\n",
    "    gdal.Warp(workspace + r'\\slope_4326.tif',slope.name,dstSRS='EPSG:4326')\n",
    "    slope_4326 = rasterio.open(workspace + r'\\slope_4326.tif')\n",
    "\n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        slope_4326.name,\n",
    "        stats=[\"majority\"],\n",
    "        prefix=\"sl_\", all_touched = True, geojson_out=True)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalizing rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def finalizing_rasters(workspace, clusters, crs):\n",
    "    output = workspace + r'\\placeholder.geojson'\n",
    "    with open(output, \"w\") as dst:\n",
    "        collection = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": list(clusters)}\n",
    "        dst.write(json.dumps(collection))\n",
    "  \n",
    "    clusters = gpd.read_file(output)\n",
    "    os.remove(output)\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def preparing_for_vectors(workspace, clusters, crs):   \n",
    "    clusters.crs = {'init' :'epsg:4326'}\n",
    "    clusters = clusters.to_crs({ 'init': crs}) \n",
    "    points = clusters.copy()\n",
    "    points[\"geometry\"] = points[\"geometry\"].centroid\n",
    "    points.to_file(workspace + r'\\clusters_cp.shp', driver='ESRI Shapefile')\n",
    "    print(datetime.datetime.now())    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_lines(name, admin, crs, workspace, clusters):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    lines=gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "\n",
    "    lines_clip = gpd.clip(lines, admin)\n",
    "    lines_clip.crs = {'init' :'epsg:4326'}\n",
    "    lines_proj=lines_clip.to_crs({ 'init': crs})\n",
    "\n",
    "    lines_proj.to_file(workspace + r\"\\ \" + name + \"_proj.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "    line = fiona.open(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    firstline = line.next()\n",
    "\n",
    "    schema = {'geometry' : 'Point', 'properties' : {'id' : 'int'},}\n",
    "    with fiona.open(workspace + r\"\\ \" + name + \"_proj_points.shp\", \"w\", \"ESRI Shapefile\", schema) as output:\n",
    "        for lines in line:\n",
    "            if lines[\"geometry\"] is not None:\n",
    "                first = shape(lines['geometry'])\n",
    "                length = first.length\n",
    "                for distance in range(0,int(length),100):\n",
    "                    point = first.interpolate(distance)\n",
    "                    output.write({'geometry' :mapping(point), 'properties' : {'id':1}})\n",
    "\n",
    "    lines_f = fiona.open(workspace + r\"\\ \" + name + \"_proj_points.shp\")\n",
    "    lines = gpd.read_file(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    points = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in lines_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "\n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    def do_kdtree(combined_x_y_arrays,points):\n",
    "        mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "        dist, indexes = mytree.query(points)\n",
    "        return dist, indexes\n",
    "\n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "\n",
    "    z=results1.tolist()\n",
    "    clusters[name+'Dist'] = z\n",
    "    clusters[name+'Dist'] = clusters[name+'Dist']/1000\n",
    "\n",
    "    a = vector_overlap(lines, clusters, name+'Dist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id',name+'Dist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "\n",
    "    del clusters[name+'Dist2']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_points(name, admin, crs, workspace, clusters, mg_filter):\n",
    "    messagebox.showinfo('OnSSET', 'Select the ' + name + ' map')\n",
    "    points=gpd.read_file(filedialog.askopenfilename(filetypes = ((\"shapefile\",\"*.shp\"),(\"all files\",\"*.*\"))))\n",
    "    if mg_filter:\n",
    "        points['umgid'] = range(0, len(points))\n",
    "        points_post = points\n",
    "\n",
    "    points_clip = gpd.clip(points, admin)\n",
    "    points_clip.crs = {'init' :'epsg:4326'}\n",
    "    points_proj=points_clip.to_crs({ 'init': crs})\n",
    "\n",
    "    points_proj.to_file(workspace + r\"\\ \" + name + \"_proj.shp\", driver='ESRI Shapefile')\n",
    "\n",
    "    points_f = fiona.open(workspace + r\"\\ \" + name + \"_proj.shp\")\n",
    "    points = gpd.read_file(workspace +  r\"\\ \" + name + \"_proj.shp\")\n",
    "    points2 = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in points_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "    \n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points2]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    def do_kdtree(combined_x_y_arrays,points):\n",
    "        mytree = scipy.spatial.cKDTree(combined_x_y_arrays)\n",
    "        dist, indexes = mytree.query(points)\n",
    "        return dist, indexes\n",
    "\n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    results1, results2 = do_kdtree(s1_arr,s2_arr)\n",
    "\n",
    "    z=results1.tolist()\n",
    "    clusters[name+'Dist'] = z\n",
    "    clusters[name+'Dist'] = clusters[name+'Dist']/1000.\n",
    "    if mg_filter:\n",
    "        z2 = results2.tolist()\n",
    "        clusters['umgid'] = z2\n",
    "\n",
    "    a = vector_overlap(points, clusters, name+'Dist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id',name+'Dist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters[name+'Dist2'] == 0, name+'Dist'] = 0\n",
    "    \n",
    "    if mg_filter:\n",
    "        clusters = pd.merge(clusters, points_post[['umgid', 'name', \"MV_network\", \"MG_type\"]], on='umgid', how = 'left')\n",
    "        clusters.rename(columns = {'name':'ESP_name',\n",
    "                                   'MV_network':'ESP_MV_status',\n",
    "                                   'MG_type':'ESP_type'}, inplace = True)\n",
    "\n",
    "    del clusters[name+'Dist2']\n",
    "    del clusters['umgid']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing hydro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processing_hydro(admin, crs, workspace, clusters, points, hydropowervalue, \n",
    "                     hydropowerunit):\n",
    "\n",
    "    points_clip = gpd.clip(points, admin)\n",
    "    points_clip.crs = {'init' :'epsg:4326'}\n",
    "    points_proj=points_clip.to_crs({ 'init': crs})\n",
    "\n",
    "    points_proj.to_file(workspace + r\"\\HydropowerDist_proj.shp\", driver='ESRI Shapefile')\n",
    "    points_f = fiona.open(workspace +  r\"\\HydropowerDist_proj.shp\")\n",
    "    points = gpd.read_file(workspace +  r\"\\HydropowerDist_proj.shp\")\n",
    "    points2 = fiona.open(workspace + r'\\clusters_cp.shp')\n",
    "\n",
    "    geoms1 = [shape(feat[\"geometry\"]) for feat in points_f]\n",
    "    s1 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms1]\n",
    "    s1_arr = np.array(s1)\n",
    "    \n",
    "    geoms2 = [shape(feat[\"geometry\"]) for feat in points2]\n",
    "    s2 = [np.array((geom.xy[0][0], geom.xy[1][0])) for geom in geoms2]\n",
    "    s2_arr = np.array(s2)\n",
    "\n",
    "    mytree = scipy.spatial.cKDTree(s1_arr)\n",
    "    dist, indexes = mytree.query(s2_arr)\n",
    "            \n",
    "    def vector_overlap(vec, settlementfile, column_name):\n",
    "        vec.drop(vec.columns.difference([\"geometry\"]), 1, inplace=True)\n",
    "        a = gpd.sjoin(settlementfile, vec, op = 'intersects')\n",
    "        a[column_name + '2'] = 0\n",
    "        return a  \n",
    "\n",
    "    z1=dist.tolist()\n",
    "    z2=indexes.tolist()\n",
    "    clusters['HydropowerDist'] = z1\n",
    "    clusters['HydropowerDist'] = clusters['HydropowerDist']/1000\n",
    "    clusters['HydropowerFID'] = z2\n",
    "    \n",
    "    z3 = []\n",
    "    for s in indexes:\n",
    "        z3.append(points[hydropowervalue][s])\n",
    "        \n",
    "    clusters['Hydropower'] = z3\n",
    "    \n",
    "    x = hydropowerunit\n",
    "    \n",
    "    if x is 'MW':\n",
    "        clusters['Hydropower'] = clusters['Hydropower']*1000\n",
    "    elif x is 'kW':\n",
    "        clusters['Hydropower'] = clusters['Hydropower']\n",
    "    else:\n",
    "        clusters['Hydropower'] = clusters['Hydropower']/1000\n",
    "\n",
    "    a = vector_overlap(points, clusters, 'HydropowerDist')\n",
    "\n",
    "    clusters = pd.merge(left = clusters, right = a[['id','HydropowerDist2']], on='id', how = 'left')\n",
    "    clusters.drop_duplicates(subset =\"id\", keep = \"first\", inplace = True) \n",
    "\n",
    "    clusters.loc[clusters['HydropowerDist2'] == 0, 'HydropowerDist'] = 0\n",
    "\n",
    "    del clusters['HydropowerDist2']\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the prioritization columns for filter visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_prio_columns(clusters):\n",
    "    if \"HF_kWh\" in clusters:\n",
    "        clusters[\"School\"] = np.where((clusters[\"HF_kWh\"] > 0), 1, 0)\n",
    "    \n",
    "    if \"EF_kWh\" in clusters:\n",
    "        clusters[\"Health_facility\"] = np.where((clusters[\"EF_kWh\"] > 0), 1, 0)\n",
    "    \n",
    "    if \"waterpoints_count\" in clusters:\n",
    "        clusters[\"Water_point\"] = np.where((clusters[\"waterpoints_count\"] > 0), 1, 0)\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def conditioning(clusters, workspace, popunit):\n",
    "    clusters = clusters.to_crs({ 'init': 'epsg:4326'}) \n",
    "\n",
    "    clusters = clusters.rename(columns={\"NightLight\": \"NightLights\", popunit : \"Pop\",})\n",
    "\n",
    "    if \"Pop_cellscount\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"ClusterCellscount\": \"ClusterCells\"})\n",
    "        \n",
    "    if \"CoreCellscount\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"CoreCellscount\": \"CoreCells\"})\n",
    "        \n",
    "    if \"landcovermajority\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"landcovermajority\": \"LandCover\"})\n",
    "\n",
    "    if \"elevationmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"elevationmean\": \"Elevation\"})  \n",
    "\n",
    "    if \"sl_majority\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"sl_majority\": \"Slope\"})\n",
    "\n",
    "    if \"ghimean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"ghimean\": \"GHI\"})\n",
    "        \n",
    "    if \"traveltimemean\" in clusters:\n",
    "        clusters[\"traveltimemean\"] = clusters[\"traveltimemean\"]/60\n",
    "        clusters = clusters.rename(columns={\"traveltimemean\": \"TravelHours\"})\n",
    "    elif \"TravelHour\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"TravelHour\": \"TravelHours\"})\n",
    "        \n",
    "    if \"windmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"windmean\": \"WindVel\"})\n",
    "    \n",
    "    if \"Residentia\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Resudentia\": \"ResidentialDemandTierCustom\"})\n",
    "    elif \"customdemandmean\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"customdemandmean\": \"ResidentialDemandTierCustom\"})\n",
    "    else:\n",
    "        clusters[\"ResidentialDemandTierCustom\"] = 0\n",
    "    \n",
    "    if \"Substation\" in clusters:\n",
    "        clusers = clusters.rename(columns={\"Substation\": \"SubstationDist\"})\n",
    "    elif \"SubstationDist\" not in clusters:\n",
    "        clusters[\"SubstationDist\"] = 99999\n",
    "\n",
    "    if \"CurrentHVL\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"CurrentHVL\": \"Existing_HVDist\"})\n",
    "    \n",
    "    if \"CurrentMVL\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"CurrentMVL\": \"Existing_MVDist\"})\n",
    "    \n",
    "    if \"PlannedHVL\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"PlannedHVL\": \"Planned_HVDist\"})\n",
    "    \n",
    "    if \"PlannedMVL\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"PlannedMVL\": \"Planned_MVDist\"})\n",
    "\n",
    "    if \"Existing_HVDist\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Existing_HVDist\": \"CurrentHVLineDist\"})\n",
    "        if \"Planned_HVDist\" in clusters:    \n",
    "            mask = (clusters['Planned_HVDist'] > clusters['CurrentHVLineDist'])\n",
    "            clusters['Planned_HVDist'][mask] = clusters['CurrentHVLineDist']\n",
    "            clusters = clusters.rename(columns={\"Planned_HVDist\": \"PlannedHVLineDist\"})\n",
    "        else:\n",
    "            clusters[\"PlannedHVLineDist\"] = clusters[\"CurrentHVLineDist\"]\n",
    "    elif \"Existing_HVDist\" not in clusters and \"Planned_HVDist\" not in clusters:\n",
    "        clusters[\"PlannedHVLineDist\"] = 99999\n",
    "        clusters[\"CurrentHVLineDist\"] = 99999\n",
    "    else:\n",
    "        clusters[\"CurrentHVLineDist\"] = 99999\n",
    "        clusters = clusters.rename(columns={\"Planned_HVDist\": \"PlannedHVLineDist\"})\n",
    "\n",
    "    if \"Existing_MVDist\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Existing_MVDist\": \"CurrentMVLineDist\"})\n",
    "        if \"Planned_MVDist\" in clusters:    \n",
    "            mask = (clusters['Planned_MVDist'] > clusters['CurrentMVLineDist'])\n",
    "            clusters['Planned_MVDist'][mask] = clusters['CurrentMVLineDist']\n",
    "            clusters = clusters.rename(columns={\"Planned_MVDist\": \"PlannedMVLineDist\"})\n",
    "        else:\n",
    "            clusters[\"PlannedMVLineDist\"] = clusters[\"CurrentMVLineDist\"]\n",
    "    elif \"Existing_MVDist\" not in clusters and \"Planned_MVDist\" not in clusters:\n",
    "        clusters[\"PlannedMVLineDist\"] = 99999\n",
    "        clusters[\"CurrentMVLineDist\"] = 99999\n",
    "    else:\n",
    "        clusters[\"CurrentMVLineDist\"] = 99999\n",
    "        clusters = clusters.rename(columns={\"Planned_MVDist\": \"PlannedMVLineDist\"})\n",
    "\n",
    "    if \"RoadsDist\" not in clusters:\n",
    "        clusters[\"RoadsDist\"] = 99999\n",
    "\n",
    "    if \"Transforme\" in clusters: \n",
    "        clusters = clusters.rename(columns={\"Transforme\": \"TransformerDist\"})\n",
    "    elif \"TransformerDist\" not in clusters:\n",
    "        clusters[\"TransformerDist\"] = 99999\n",
    "\n",
    "    if \"Hydropower\" not in clusters:\n",
    "        clusters[\"Hydropower\"] = 0\n",
    "        \n",
    "    if \"Hydropow_1\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Hydropow_1\": \"HydropowerDist\"})\n",
    "    elif 'HydropowerDist' not in clusters:\n",
    "        clusters[\"HydropowerDist\"] = 99999\n",
    "        \n",
    "    if \"Hydropow_2\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"Hydropow_2\": \"HydropowerFID\"})\n",
    "    elif \"HydropowerFID\" not in clusters:\n",
    "        clusters[\"HydropowerFID\"] = 0\n",
    "    \n",
    "    if \"IsUrban\" not in clusters:\n",
    "        clusters[\"IsUrban\"] = 0    \n",
    "        \n",
    "    if \"PerCapitaD\" not in clusters:\n",
    "        clusters[\"PerCapitaDemand\"] = 0\n",
    "    else:\n",
    "        clusters = clusters.rename(columns={\"PerCapitaD\": \"PerCapitaDemand\"})\n",
    "        \n",
    "    if \"HealthDema\" not in clusters:\n",
    "        clusters[\"HealthDemand\"] = 0     \n",
    "    else:\n",
    "        clusters = clusters.rename(columns={\"HealthDema\": \"HealthDemand\"})\n",
    "        \n",
    "    if \"EducationD\" not in clusters:\n",
    "        clusters[\"EducationDemand\"] = 0     \n",
    "    else:\n",
    "        clusters = clusters.rename(columns={\"EducationD\": \"EducationDemand\"})   \n",
    "        \n",
    "    if \"AgriDemand\" not in clusters:\n",
    "        clusters[\"AgriDemand\"] = 0  \n",
    "        \n",
    "    if \"Commercial\" not in clusters:\n",
    "        clusters[\"CommercialDemand\"] = 0\n",
    "    else:\n",
    "        clusters = clusters.rename(columns={\"Commercial\": \"CommercialDemand\"})\n",
    "        \n",
    "    if \"Conflict\" not in clusters:\n",
    "        clusters[\"Conflict\"] = 0       \n",
    "\n",
    "    if \"Electrific\" not in clusters:\n",
    "        clusters[\"ElectrificationOrder\"] = 0\n",
    "    else:\n",
    "        clusters = clusters.rename(columns={\"Electrific\": \"ElectrificationOrder\"})\n",
    "    \n",
    "    if \"Resident_1\" not in clusters:\n",
    "        clusters[\"ResidentialDemandTier1\"] = 7.74\n",
    "    else: \n",
    "        clusters = clusters.rename(columns={\"Resident_1\": \"ResidentialDemandTier1\"})\n",
    "\n",
    "    if \"Resident_2\" not in clusters:\n",
    "        clusters[\"ResidentialDemandTier2\"] = 43.8\n",
    "    else: \n",
    "        clusters = clusters.rename(columns={\"Resident_2\": \"ResidentialDemandTier2\"})\n",
    "\n",
    "    if \"Resident_3\" not in clusters:\n",
    "        clusters[\"ResidentialDemandTier3\"] = 160.6\n",
    "    else: \n",
    "        clusters = clusters.rename(columns={\"Resident_3\": \"ResidentialDemandTier3\"})\n",
    "\n",
    "    if \"Resident_4\" not in clusters:\n",
    "        clusters[\"ResidentialDemandTier4\"] = 423.4\n",
    "    else: \n",
    "        clusters = clusters.rename(columns={\"Resident_4\": \"ResidentialDemandTier4\"})\n",
    "    \n",
    "    if \"Resident_5\" not in clusters:\n",
    "        clusters[\"ResidentialDemandTier5\"] = 598.6\n",
    "    else: \n",
    "        clusters = clusters.rename(columns={\"Resident_5\": \"ResidentialDemandTier5\"})\n",
    "        \n",
    "    if \"Existing_ESP_Dist\" not in clusters:\n",
    "        clusters[\"Existing_ESP_Dist\"] = 99999\n",
    "    \n",
    "    if \"ESP_name\" not in clusters:\n",
    "        clusters[\"ESP_name\"] = None\n",
    "        \n",
    "    if \"ESP_MV_status\" not in clusters:\n",
    "        clusters[\"ESP_MV_status\"] = None\n",
    "        \n",
    "    if \"ESP_type\" not in clusters:\n",
    "        clusters[\"ESP_type\"] = None\n",
    "        \n",
    "    if \"waterpoints_count\" in clusters:\n",
    "        clusters = clusters.rename(columns={\"waterpoints_count\": \"waterpoints\"})\n",
    "    \n",
    "    clusters[\"X_deg\"] = clusters.geometry.centroid.x\n",
    "    \n",
    "    clusters[\"Y_deg\"] = clusters.geometry.centroid.y\n",
    "    \n",
    "    #clusters.to_file(workspace + r\"\\output.shp\", driver='ESRI Shapefile')\n",
    "    clusters.to_file(workspace + r\"\\output.csv\", driver='CSV')\n",
    "    print(datetime.datetime.now())\n",
    "    return clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
